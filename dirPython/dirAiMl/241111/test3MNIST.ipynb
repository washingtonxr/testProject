{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # Import NumPy library\n",
    "import pandas as pd  # Import Pandas library\n",
    "# Solution 1\n",
    "# from keras.datasets import mnist  # Import MNIST dataset from Keras\n",
    "\n",
    "# # Load the training and testing datasets\n",
    "# (X_train_image, y_train_label), (X_test_image, y_test_label) = mnist.load_data()\n",
    "\n",
    "# print (\"数据集张量形状：\", X_train_image.shape) #用shape方法显示张量的形状\n",
    "# print (\"第一个数据样本：\\n\", X_train_image[0]) #注意Python的索引是从0开始的\n",
    "# print (\"第一个数据样本的标签：\", y_train_label[0])\n",
    "\n",
    "# Solution 2, Load local dataset\n",
    "import os\n",
    "os.listdir('../../../../dirRawDataSet')\n",
    "\n",
    "with np.load('../../../../dirRawDataSet/mnist.npz') as data:\n",
    "    X_train_image = data['x_train']\n",
    "    y_train_label = data['y_train']\n",
    "    X_test_image = data['x_test']\n",
    "    y_test_label = data['y_test']\n",
    "\n",
    "# Check the shapes of the loaded arrays\n",
    "print(\"X_train_image shape:\", X_train_image.shape)\n",
    "print(\"y_train_label shape:\", y_train_label.shape)\n",
    "print(\"X_test_image shape:\", X_test_image.shape)\n",
    "print(\"y_test_label shape:\", y_test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 1\n",
    "# from keras.utils import to_categorical # 导入keras.utils工具库的类别转换工具\n",
    "# X_train = X_train_image.reshape(60000, 28, 28, 1) # 给标签增加一个维度\n",
    "# X_test = X_test_image.reshape(10000, 28, 28, 1) # 给标签增加一个维度\n",
    "# y_train = to_categorical(y_train_label, 10) # 特征转换为one-hot编码\n",
    "# y_test = to_categorical(y_test_label, 10) # 特征转换为one-hot编码\n",
    "# print (\"训练集张量形状：\", X_train.shape) # 训练集张量的形状\n",
    "# print (\"第一个数据标签：\", y_train[0]) # 显示标签集的第一个数据\n",
    "\n",
    "# Solution 2\n",
    "from keras.utils import to_categorical  # Import the utility for one-hot encoding labels\n",
    "\n",
    "# Reshape images to include the channel dimension for compatibility with Conv2D layers\n",
    "X_train = X_train_image.reshape(-1, 28, 28, 1)  # Automatically determines the first dimension\n",
    "X_test = X_test_image.reshape(-1, 28, 28, 1)\n",
    "\n",
    "# Convert labels to one-hot encoding for categorical classification\n",
    "y_train = to_categorical(y_train_label, num_classes=10)\n",
    "y_test = to_categorical(y_test_label, num_classes=10)\n",
    "\n",
    "# Display the shapes for confirmation\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"First training label (one-hot):\", y_train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 1\n",
    "# from keras import models # 导入Keras模型, 以及各种神经网络的层\n",
    "# from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "# model = models.Sequential() # 用序贯方式建立模型\n",
    "# model.add(Conv2D(32, (3, 3), activation='relu', # 添加Conv2D层\n",
    "#                 input_shape=(28, 28, 1)))  # 指定输入数据样本张量的类型\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2))) # 添加MaxPooling2D层\n",
    "# model.add(Conv2D(64, (3, 3), activation='relu')) # 添加Conv2D层\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2))) # 添加MaxPooling2D层\n",
    "# model.add(Dropout(0.25)) # 添加Dropout层\n",
    "# model.add(Flatten()) # 展平\n",
    "# model.add(Dense(128, activation='relu')) # 添加全连接层\n",
    "# model.add(Dropout(0.5)) # 添加Dropout层\n",
    "# model.add(Dense(10, activation='softmax')) # Softmax分类激活, 输出10维分类码\n",
    "# # 编译模型\n",
    "# model.compile(optimizer='rmsprop', # 指定优化器\n",
    "# loss='categorical_crossentropy', # 指定损失函数\n",
    "# metrics=['accuracy']) # 指定验证过程中的评估指标\n",
    "\n",
    "# Solution 2\n",
    "from keras import models  # Import Keras models\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, Input\n",
    "\n",
    "model = models.Sequential()  # Create a Sequential model\n",
    "model.add(Input(shape=(28, 28, 1)))  # Define the input shape explicitly\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))  # Add Conv2D layer\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))  # Add MaxPooling2D layer\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))  # Add Conv2D layer\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))  # Add MaxPooling2D layer\n",
    "model.add(Dropout(0.25))  # Add Dropout layer\n",
    "model.add(Flatten())  # Flatten the input\n",
    "model.add(Dense(128, activation='relu'))  # Add fully connected layer\n",
    "model.add(Dropout(0.5))  # Add Dropout layer\n",
    "model.add(Dense(10, activation='softmax'))  # Output layer with softmax for 10 classes\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='rmsprop',  # Specify the optimizer\n",
    "    loss='categorical_crossentropy',  # Specify the loss function\n",
    "    metrics=['accuracy']  # Specify metrics for evaluation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code builds a Convolutional Neural Network (CNN) using Keras, iteratively creating multiple models and configuring their layers. Let's break it down:\n",
    "General Structure:\n",
    "\n",
    "    The code defines a list model that will contain multiple CNN models (15 models in total, based on the nets = 15 line).\n",
    "    For each of the 15 models, the code builds a CNN architecture with multiple convolutional layers, batch normalization, dropout layers, and fully connected (dense) layers.\n",
    "    It also compiles the model using the Adam optimizer and categorical cross-entropy loss function, which is typically used for multi-class classification tasks.\n",
    "\n",
    "Code Explanation:\n",
    "\n",
    "    Model Initialization:\n",
    "\n",
    "nets = 15\n",
    "model = [0] * nets\n",
    "\n",
    "    The nets = 15 specifies the number of CNN models to create.\n",
    "    The list model is initialized with 15 elements, each set to 0. These will hold the individual models.\n",
    "\n",
    "Iterative Model Construction:\n",
    "\n",
    "for j in range(nets):\n",
    "    model[j] = Sequential()\n",
    "\n",
    "    A loop is used to create each model (Sequential()).\n",
    "    Each model is initialized as a Keras Sequential model, which means the layers will be stacked in order.\n",
    "\n",
    "Convolutional Layers with Batch Normalization:\n",
    "\n",
    "model[j].add(Conv2D(32, kernel_size = 3, activation='relu', input_shape = (28, 28, 1)))\n",
    "model[j].add(BatchNormalization())\n",
    "model[j].add(Conv2D(32, kernel_size = 3, activation='relu'))\n",
    "model[j].add(BatchNormalization())\n",
    "model[j].add(Conv2D(32, kernel_size = 5, strides=2, padding='same', activation='relu'))\n",
    "model[j].add(BatchNormalization())\n",
    "\n",
    "    Conv2D(32, kernel_size=3, activation='relu'): Adds a convolutional layer with 32 filters (kernels), a kernel size of 3x3, and ReLU activation.\n",
    "    BatchNormalization(): Normalizes the output of the previous layer to speed up training and reduce overfitting.\n",
    "    Another Conv2D layer with a kernel size of 5x5 and strides=2 (which reduces the spatial dimensions of the input).\n",
    "    padding='same' ensures that the output size matches the input size by padding the input.\n",
    "    These layers extract features from the image data (28x28x1, grayscale images).\n",
    "\n",
    "Dropout Layer:\n",
    "\n",
    "model[j].add(Dropout(0.4))\n",
    "\n",
    "    This layer randomly sets 40% of the inputs to zero during training to prevent overfitting.\n",
    "\n",
    "Additional Convolutional and Pooling Layers:\n",
    "\n",
    "model[j].add(Conv2D(64, kernel_size = 3, activation='relu'))\n",
    "model[j].add(BatchNormalization())\n",
    "model[j].add(Conv2D(64, kernel_size = 3, activation='relu'))\n",
    "model[j].add(BatchNormalization())\n",
    "model[j].add(Conv2D(64, kernel_size = 5, strides=2, padding='same', activation='relu'))\n",
    "model[j].add(BatchNormalization())\n",
    "model[j].add(Dropout(0.4))\n",
    "\n",
    "    After the first set of convolutional layers, the code adds more Conv2D layers with 64 filters.\n",
    "    Again, each convolutional layer is followed by batch normalization and dropout layers.\n",
    "    These layers are used to learn more complex features and further reduce overfitting.\n",
    "\n",
    "Final Convolutional Layer and Flatten:\n",
    "\n",
    "model[j].add(Conv2D(128, kernel_size = 4, activation='relu'))\n",
    "model[j].add(BatchNormalization())\n",
    "model[j].add(Flatten())\n",
    "model[j].add(Dropout(0.4))\n",
    "\n",
    "    The final Conv2D layer has 128 filters and uses a kernel size of 4x4.\n",
    "    The output is flattened to a 1D array, as a fully connected (dense) layer will be applied next.\n",
    "    Another dropout layer is added.\n",
    "\n",
    "Fully Connected Output Layer:\n",
    "\n",
    "model[j].add(Dense(10, activation='softmax'))\n",
    "\n",
    "    The Dense layer with 10 neurons is the output layer, which corresponds to the number of classes (10 classes for MNIST digits).\n",
    "    The Softmax activation function is used, which will output a probability distribution across the 10 classes.\n",
    "\n",
    "Model Compilation:\n",
    "\n",
    "    model[j].compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "        The model is compiled using the Adam optimizer, which adapts the learning rate based on training performance.\n",
    "        The categorical cross-entropy loss function is used because this is a multi-class classification task (MNIST).\n",
    "        The model will be evaluated using accuracy as the metric.\n",
    "\n",
    "Summary:\n",
    "\n",
    "This code builds 15 different CNN models with the following structure:\n",
    "\n",
    "    Multiple convolutional layers with Batch Normalization and Dropout for regularization.\n",
    "    Pooling layers (via strides) to reduce the spatial dimensions.\n",
    "    A final Dense layer with 10 units for classification into 10 categories (digits from 0 to 9).\n",
    "    The models are compiled using the Adam optimizer and categorical cross-entropy loss function.\n",
    "\n",
    "The code is designed to create and compile 15 distinct models, which could then be trained on the MNIST dataset or another similar dataset. The idea behind having multiple models may be for ensemble learning, where each model's predictions are combined to improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 1\n",
    "# model.fit(X_train, y_train, # 指定训练特征集和训练标签集\n",
    "#             validation_split = 0.3, # 部分训练集数据拆分成验证集\n",
    "#             epochs=5, # 训练轮次为5轮\n",
    "#             batch_size=128) # 以128为批量进行训练\n",
    "\n",
    "# Solution 2\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau  # Import callbacks\n",
    "\n",
    "# Set up callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),  # Stop early if no improvement in 3 epochs\n",
    "    ModelCheckpoint('best_model.keras', monitor='val_accuracy', save_best_only=True),  # Save the best model\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=1e-5)  # Reduce learning rate on plateau\n",
    "]\n",
    "\n",
    "# Train the model with the optimized settings\n",
    "history = model.fit(\n",
    "    X_train, y_train,                  # Specify training features and labels\n",
    "    validation_split=0.3,              # Split 30% of training data for validation\n",
    "    epochs=5,                          # Set higher max epochs, but with early stopping\n",
    "    batch_size=128,                    # Use a batch size of 128\n",
    "    callbacks=callbacks,               # Use callbacks for early stopping, model checkpoint, and learning rate adjustment\n",
    "    verbose=1                          # Print training progress\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 1\n",
    "# score = model.evaluate(X_test, y_test) # 在验证集上进行模型评估\n",
    "# print('测试集预测准确率:', score[1]) # 输出测试集上的\n",
    "\n",
    "# Solution 2\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)  # Evaluate with minimal output\n",
    "print(f\"测试集预测准确率: {test_accuracy:.4f}\")  # Format accuracy to 4 decimal places\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 1\n",
    "# pred = model.predict(X_test[0].reshape(1, 28, 28, 1)) # 预测测试集第一个数据\n",
    "# print(pred[0], \"转换一下格式得到：\", pred.argmax()) # 把one-hot编码转换为数字\n",
    "# import matplotlib.pyplot as plt # 导入绘图工具包\n",
    "# plt.imshow(X_test[0].reshape(28, 28), cmap='Greys') # 输出这个图片\n",
    "\n",
    "# Solution 2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "# Set the font to a CJK-compatible font\n",
    "# plt.rcParams['font.family'] = 'Noto Sans CJK JP'\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "\n",
    "# Your code to make predictions and plot\n",
    "first_prediction = model.predict(X_test[0].reshape(1, 28, 28, 1))\n",
    "predicted_label = first_prediction.argmax()                           \n",
    "\n",
    "# Display prediction result and image\n",
    "print(f\"预测值 (one-hot 编码): {first_prediction[0]}, 转换为数字格式: {predicted_label}\")\n",
    "plt.imshow(X_test[0], cmap='Greys')\n",
    "plt.title(f\"Model prediction: {predicted_label}\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "plt.rcdefaults()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
